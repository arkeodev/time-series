{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/arkeodev/time-series/blob/main/Statistical_Time_Series_Analysis/10-aic-vs-bic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. AIC vs BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AIC (Akaike Information Criterion)** and **BIC (Bayesian Information Criterion)** are two commonly used metrics for model selection in statistics and machine learning. They help in comparing different models (e.g., different orders of an AR model) by balancing two competing factors:\n",
    "\n",
    "1. **Goodness of Fit**: How well does the model fit the data?\n",
    "2. **Model Complexity**: How many parameters does the model have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Akaike Information Criterion (AIC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{AIC} = 2k - 2 \\ln(L)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $( k $ is the number of estimated parameters in the model.\n",
    "- $( L $ is the maximized value of the likelihood function for the model (often given as $\\ln(L)$).\n",
    "\n",
    "**Interpretation**:\n",
    "- **Lower AIC values** indicate a better trade-off of model fit versus complexity.\n",
    "- The AIC penalizes complexity less strongly than the BIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bayesian Information Criterion (BIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{BIC} = k \\ln(n) - 2 \\ln(L)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $( k $ is the number of parameters.\n",
    "- $( n $ is the number of observations.\n",
    "- $( L $ is the maximized likelihood function.\n",
    "\n",
    "**Interpretation**:\n",
    "- **Lower BIC values** also indicate a better model.\n",
    "- BIC penalizes complexity more strongly than AIC (notice the $\\ln(n)$ factor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Why Use AIC or BIC?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Comparing Models**: Both criteria allow you to compare multiple models (e.g., AR(1), AR(2), â€¦, AR(p)) on a consistent scale.\n",
    "2. **Penalize Overfitting**: As you add more parameters, the likelihood typically improves, but AIC/BIC counterbalance this by penalizing complexity.\n",
    "3. **Model Selection**: You usually pick the model with the **lowest AIC or BIC**.  \n",
    "   - AIC tends to favor slightly more complex models.  \n",
    "   - BIC, with a stronger penalty on parameters, tends to pick more parsimonious (simpler) models.\n",
    "\n",
    "In time series analysis (for example, comparing different ARMA/ARIMA models), you will often see **AIC** or **BIC** used to guide the choice of model order."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
